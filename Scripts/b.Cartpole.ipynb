{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartpole DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :50, score : 9.7, n_buffer : 485, eps : 7.8%\n",
      "n_episode :100, score : 9.7, n_buffer : 972, eps : 7.5%\n",
      "n_episode :150, score : 9.8, n_buffer : 1462, eps : 7.3%\n",
      "n_episode :200, score : 9.9, n_buffer : 1956, eps : 7.0%\n",
      "n_episode :250, score : 11.0, n_buffer : 2508, eps : 6.8%\n",
      "n_episode :300, score : 9.8, n_buffer : 2999, eps : 6.5%\n",
      "n_episode :350, score : 11.0, n_buffer : 3549, eps : 6.2%\n",
      "n_episode :400, score : 12.7, n_buffer : 4185, eps : 6.0%\n",
      "n_episode :450, score : 27.8, n_buffer : 5573, eps : 5.8%\n",
      "n_episode :500, score : 85.3, n_buffer : 9837, eps : 5.5%\n",
      "n_episode :550, score : 75.7, n_buffer : 13620, eps : 5.3%\n",
      "n_episode :600, score : 166.0, n_buffer : 21920, eps : 5.0%\n",
      "n_episode :650, score : 311.2, n_buffer : 37480, eps : 4.8%\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size    = 32\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n): # 버퍼에서 샘플링\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)   \n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x): # Q Value 리턴 (음수가 될 수 도 있음)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))    \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "      \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random() # 0 ~ 1 \n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else : \n",
    "            return out.argmax().item()\n",
    "            \n",
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s) # input size (32,4) return size (32,2)\n",
    "        q_a = q_out.gather(1, a) # 취한 액션의 큐값만 골라냄 (32,1)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "q = Qnet()\n",
    "q_target = Qnet()\n",
    "q_target.load_state_dict(q.state_dict()) # Copy network weights\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "print_interval = 50\n",
    "score = 0.0  \n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate) # No weight updates - q_target\n",
    "\n",
    "for n_epi in range(2000):\n",
    "    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n",
    "        s_prime, r, done, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((s, a, r/100.0, s_prime, done_mask))\n",
    "        s = s_prime\n",
    "\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if memory.size()>2000:\n",
    "        train(q, q_target, memory, optimizer)\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        q_target.load_state_dict(q.state_dict()) # target network updates for every 50 episodes\n",
    "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score/print_interval, memory.size(), epsilon*100))                \n",
    "        \n",
    "        if (score/print_interval) > 300:\n",
    "            break\n",
    "            \n",
    "        score = 0.0\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "path = 'cartpole.pt'\n",
    "# torch.save(q_target.state_dict(), path) # save weights only\n",
    "# torch.save(q_target, path)\n",
    "q_target = torch.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### RL Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Episode finished after 431 timesteps\n",
      "Episode finished after 500 timesteps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i_episode in range(3):\n",
    "    observation = env.reset()\n",
    "    for t in range(550):\n",
    "        time.sleep(0.02)\n",
    "        env.render()\n",
    "        action = q_target(torch.Tensor(observation)).argmax().item() \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            time.sleep(1)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Random or Simple Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 10 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 33 timesteps\n",
      "Episode finished after 32 timesteps\n",
      "Episode finished after 33 timesteps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    for t in range(300):\n",
    "        time.sleep(0.1)\n",
    "        env.render()\n",
    "        \n",
    "#         action = env.action_space.sample()\n",
    "#         action = random.randint(0,1)\n",
    "\n",
    "        if observation[0] < 0: # if the pole is on the right side\n",
    "            action = 1 #  pushing the cart to the left\n",
    "        else:\n",
    "            action = 0 #  pushing the cart to the right\n",
    " \n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            time.sleep(1)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n"
     ]
    }
   ],
   "source": [
    "def theta_omega_policy(obs):\n",
    "    theta, w = obs[2:4]\n",
    "    if abs(theta) < 0.03:\n",
    "        return 0 if w < 0 else 1\n",
    "    else:\n",
    "        return 0 if theta < 0 else 1\n",
    "    \n",
    "import time\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "for i_episode in range(3):\n",
    "    observation = env.reset()\n",
    "    for t in range(550):\n",
    "        time.sleep(0.01)\n",
    "        env.render()\n",
    "        action = theta_omega_policy(observation)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            time.sleep(1)\n",
    "            break\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Keep Pushing to the left or Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting states\n",
      "[-0.04456399  0.04653909  0.01326909 -0.02099827]\n",
      "\n",
      "\n",
      "0 action: 0 states: [-0.04363321 -0.14877062  0.01284913  0.2758415 ] reward: 1.0\n",
      "1 action: 0 states: [-0.04660862 -0.3440735   0.01836596  0.5725492 ] reward: 1.0\n",
      "2 action: 0 states: [-0.05349009 -0.5394481   0.02981694  0.87096095] reward: 1.0\n",
      "3 action: 0 states: [-0.06427906 -0.73496264  0.04723616  1.1728673 ] reward: 1.0\n",
      "4 action: 0 states: [-0.07897831 -0.93066573  0.07069351  1.4799768 ] reward: 1.0\n",
      "5 action: 0 states: [-0.09759162 -1.1265757   0.10029304  1.7938743 ] reward: 1.0\n",
      "6 action: 0 states: [-0.12012314 -1.3226682   0.13617052  2.1159716 ] reward: 1.0\n",
      "7 action: 0 states: [-0.1465765  -1.5188614   0.17848997  2.4474478 ] reward: 1.0\n",
      "8 action: 0 states: [-0.17695373 -1.7149992   0.22743891  2.7891784 ] reward: 1.0\n",
      "9 action: 0 states: [-0.21125372 -1.9108318   0.2832225   3.1416543 ] reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "\n",
    "env.seed(0)\n",
    "\n",
    "print('starting states')\n",
    "s = env.reset()\n",
    "print(s)\n",
    "print('\\n')\n",
    "\n",
    "a = 0\n",
    "for i in range(10):\n",
    "    time.sleep(0.1)\n",
    "    env.render()\n",
    "    s_prime, r, done, info = env.step(a)\n",
    "    print(i, 'action:', a, 'states:' , s_prime, 'reward:', r)\n",
    "    \n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting states\n",
      "[-0.04456399  0.04653909  0.01326909 -0.02099827]\n",
      "\n",
      "\n",
      "0 action: 1 states: [-0.04363321  0.24146827  0.01284913 -0.3094653 ] reward: 1.0\n",
      "1 action: 1 states: [-0.03880385  0.4364048   0.00665982 -0.5980684 ] reward: 1.0\n",
      "2 action: 1 states: [-0.03007575  0.63143295 -0.00530154 -0.8886461 ] reward: 1.0\n",
      "3 action: 1 states: [-0.01744709  0.8266264  -0.02307447 -1.1829909 ] reward: 1.0\n",
      "4 action: 1 states: [-9.1456366e-04  1.0220401e+00 -4.6734285e-02 -1.4828167e+00] reward: 1.0\n",
      "5 action: 1 states: [ 0.01952624  1.2176999  -0.07639062 -1.7897208 ] reward: 1.0\n",
      "6 action: 1 states: [ 0.04388024  1.4135911  -0.11218503 -2.1051378 ] reward: 1.0\n",
      "7 action: 1 states: [ 0.07215206  1.6096447  -0.15428779 -2.4302828 ] reward: 1.0\n",
      "8 action: 1 states: [ 0.10434495  1.8057201  -0.20289344 -2.766083  ] reward: 1.0\n",
      "9 action: 1 states: [ 0.14045936  2.0015864  -0.2582151  -3.1130984 ] reward: 1.0\n",
      "10 action: 1 states: [ 0.18049109  2.196903   -0.32047707 -3.4714346 ] reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "env.seed(0)\n",
    "\n",
    "s = env.reset()\n",
    "print('starting states')\n",
    "print(s)\n",
    "print('\\n')\n",
    "\n",
    "a = 1\n",
    "for i in range(11):\n",
    "    time.sleep(0.1)\n",
    "    env.render()\n",
    "    s_prime, r, done, info = env.step(a)\n",
    "    print(i, 'action:', a, 'states:' , s_prime, 'reward:', r)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
